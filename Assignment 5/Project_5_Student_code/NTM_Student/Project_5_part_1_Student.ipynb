{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_5_part_1_Student.ipynb","provenance":[{"file_id":"17BvkhrvTKBi0IMM1Fazl6UVLtm0fb8E7","timestamp":1573460220489},{"file_id":"1tRHaZoIYsfB16Gh-s_KAX6A41NdP3XKa","timestamp":1573332369516},{"file_id":"1EfocEfdlVns48iXbXJf7_Jy-v-WCN9NN","timestamp":1573327062889}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7qpkAadhgi56"},"source":["#Name: Justin Helfman"]},{"cell_type":"markdown","metadata":{"id":"Ep1Ky-nDBZIr"},"source":["You might need to modify the third line in the code cell below, to make sure you cd to the actual directory which your ipynb file is located in.\n","\n","**Caution**: due to the nature of this project's setup, everytime you want to rerun some code cell below, please click **Runtime -> Restart and run all**; this operation clears the computational graphs and the local variables but allow training and testing data that are already loaded from google drive to stay in the colab runtime space. Please do **not** do the following if you just wish to rerun code: click Runtime -> reset all runtimes, and then click Runtime -> Run all; it will remount your google drive, and remove the training and testing data already loaded in your colab runtime space. **Runtime -> Restart and run all** automatically avoids remounting the drive after the first time you run the notebook file; the loaded data can usually stay in your colab runtime space for many hours.\n","\n","Loading the training and testing data after remounting your google drive takes 30 - 40 minutes."]},{"cell_type":"code","metadata":{"id":"7sWni0FseVUz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606846013587,"user_tz":300,"elapsed":71721,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}},"outputId":"1a4386f5-ced8-4986-dd83-f3b83cdc4e40"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\", force_remount=True)\n","%cd drive/My Drive/ECE 595/Assignment 5/Project_5_Student_code/NTM_Student\n","%cd Neural_Turing_Machine/NTM_small #These directories are not present (at least in the zip file I got)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/My Drive/ECE 595/Assignment 5/Project_5_Student_code/NTM_Student\n","[Errno 2] No such file or directory: 'Neural_Turing_Machine/NTM_small #These directories are not present (at least in the zip file I got)'\n","/content/drive/My Drive/ECE 595/Assignment 5/Project_5_Student_code/NTM_Student\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R9gdekJg_-xa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606846015663,"user_tz":300,"elapsed":73784,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}},"outputId":"28a241de-c1a9-4c8d-fb21-15d00599be8d"},"source":["from utils import OmniglotDataLoader, one_hot_decode, five_hot_decode\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import argparse\n","import numpy as np\n","%tensorflow_version 1.x\n","print(tf.__version__)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","TensorFlow is already loaded. Please restart the runtime to change versions.\n","2.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"otNm4yidAQQB"},"source":["Already implemented, no need to change.\n","\n","This class is part of the training loop."]},{"cell_type":"code","metadata":{"id":"MZTXPodW_5_i","executionInfo":{"status":"ok","timestamp":1606846015665,"user_tz":300,"elapsed":73779,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}}},"source":["class NTMOneShotLearningModel():\n","  def __init__(self, model, n_classes, batch_size, seq_length, image_width, image_height,\n","                rnn_size, num_memory_slots, rnn_num_layers, read_head_num, write_head_num, memory_vector_dim, learning_rate):\n","    self.output_dim = n_classes\n","\n","    # Note: the images are flattened to 1D tensors\n","    # The input data structure is of the following form:\n","    # self.x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","    self.x_image = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, image_width * image_height])\n","    # Model's output label is one-hot encoded\n","    # The data structure is of the following form:\n","    # self.x_label[i,j,:] = one-hot label of the jth image in \n","    #             the ith sequence (or, episode)\n","    self.x_label = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","    # Target label is one-hot encoded\n","    self.y = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","    \n","    # The dense layer for mapping controller output and retrieved\n","    # memory content to classification labels\n","    self.controller_output_to_ntm_output = tf.keras.layers.Dense(units=self.output_dim, use_bias=True)\n","\n","    if model == 'LSTM':\n","      # Using a LSTM layer to serve as the controller, no memory\n","      def rnn_cell(rnn_size):\n","        return tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n","      cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(rnn_size) for _ in range(rnn_num_layers)])\n","      state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n","    \n","    # Initialize the controller model, including wiping its memory\n","    # Also, get the initial state of the MANN model\n","    \n","    self.state_list = [state]\n","    # Setup the NTM's output\n","    self.o = []\n","    \n","    # Now iterate over every sample in the sequence \n","    for t in range(seq_length):\n","      output, state = cell(tf.concat([self.x_image[:, t, :], self.x_label[:, t, :]], axis=1), state)\n","      # Map controller output (with retrieved memory) + current (offseted) label \n","      # to the overall ntm's output with an affine operation\n","      # The output is the classification labels\n","      output = self.controller_output_to_ntm_output(output)\n","      output = tf.nn.softmax(output, axis=1)\n","      self.o.append(output)\n","      self.state_list.append(state)\n","    # post-process the output of the classifier\n","    self.o = tf.stack(self.o, axis=1)\n","    self.state_list.append(state)\n","\n","    eps = 1e-8\n","    # cross entropy, between model output labels and target labels\n","    self.learning_loss = -tf.reduce_mean(  \n","        tf.reduce_sum(self.y * tf.log(self.o + eps), axis=[1, 2])\n","    )\n","    \n","    self.o = tf.reshape(self.o, shape=[batch_size, seq_length, -1])\n","    self.learning_loss_summary = tf.summary.scalar('learning_loss', self.learning_loss)\n","\n","    with tf.variable_scope('optimizer'):\n","      self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","      self.train_op = self.optimizer.minimize(self.learning_loss)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_qMlbTWAvg0"},"source":["The training and testing functions"]},{"cell_type":"code","metadata":{"id":"Se1yEaxmey6Z","executionInfo":{"status":"ok","timestamp":1606846015665,"user_tz":300,"elapsed":73774,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}}},"source":["def train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir):\n","  \n","  # We always use one-hot encoding of the labels in this experiment\n","  label_type = \"one_hot\"\n","\n","  # Initialize the model\n","  model = NTMOneShotLearningModel(model=model_path, n_classes=n_classes,\\\n","                    batch_size=batch_size, seq_length=seq_length,\\\n","                    image_width=image_width, image_height=image_height, \\\n","                    rnn_size=rnn_size, num_memory_slots=num_memory_slots,\\\n","                    rnn_num_layers=rnn_num_layers, read_head_num=read_head_num,\\\n","                    write_head_num=write_head_num, memory_vector_dim=memory_vector_dim,\\\n","                    learning_rate=learning_rate)\n","  print(\"Model initialized\")\n","  data_loader = OmniglotDataLoader(\n","      image_size=(image_width, image_height),\n","      n_train_classses=n_train_classes,\n","      n_test_classes=n_test_classes\n","  )\n","  print(\"Data loaded\")\n","  # Note: our training loop is in the tensorflow 1.x style\n","  with tf.Session() as sess:\n","    if restore_training:\n","      saver = tf.train.Saver()\n","      ckpt = tf.train.get_checkpoint_state(save_dir + '/' + model_path)\n","      saver.restore(sess, ckpt.model_checkpoint_path)\n","    else:\n","      saver = tf.train.Saver(tf.global_variables())\n","      tf.global_variables_initializer().run()\n","    train_writer = tf.summary.FileWriter(tensorboard_dir + '/' + model_path, sess.graph)\n","    print(\"1st\\t2nd\\t3rd\\t4th\\t5th\\t6th\\t7th\\t8th\\t9th\\t10th\\tepoch\\tloss\")\n","    for b in range(num_epochs):\n","      # Test the model\n","      if b % 100 == 0:\n","        # Note: the images are flattened to 1D tensors\n","        # The input data structure is of the following form:\n","        # x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","        # And the sequence of 50 images x_image[i,:,:] constitute\n","        # one episode, and each class (out of 5 classes) has around 10\n","        # appearances in this sequence, as seq_length = 50 and \n","        # n_classes = 5, as specified in the code block below\n","        # See the details in utils.py, OmniglotDataLoader class\n","        x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length,\n","                                  type='test',\n","                                  augment=augment,\n","                                  label_type=label_type)\n","        feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","        output, learning_loss = sess.run([model.o, model.learning_loss], feed_dict=feed_dict)\n","        merged_summary = sess.run(model.learning_loss_summary, feed_dict=feed_dict)\n","        train_writer.add_summary(merged_summary, b)\n","        accuracy = test(seq_length, y, output)\n","        for accu in accuracy:\n","          print('%.4f' % accu, end='\\t')\n","        print('%d\\t%.4f' % (b, learning_loss))\n","\n","      # Save model per 2000 epochs\n","      if b%2000==0 and b>0:\n","        saver.save(sess, save_dir + '/' + model_path + '/model.tfmodel', global_step=b)\n","\n","      # Train the model\n","      x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length, \\\n","                                type='train',\n","                                augment=augment,\n","                                label_type=label_type)\n","      feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","      sess.run(model.train_op, feed_dict=feed_dict)\n","      \n","# Fill in this function. You might not need seq_length (the length of an episode)\n","# as an input, depending on your setup \n","# Note: y is the true labels, and of shape (batch_size, seq_length, 5)\n","# output is the network's classification labels\n","def test(seq_length, y, output):\n","  # Fill in\n","  c = [[] * seq_length] * 10\n","\n","  i = 0\n","\n","  for i in range(10):\n","    for j in range(seq_length):\n","      mxdex = list(y[i][j]).index(min(y[i][j]))\n","      if output[i][j][mxdex] == 1:\n","        c[i].append(1)\n","      else:\n","        c[i].append(0)\n","\n","  acc = [sum(c[j])/sum(sum(y[j])) for j in range(10)]\n","\n","  return acc"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VruOInLHkZUK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606848317269,"user_tz":300,"elapsed":2375368,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}},"outputId":"92fec033-9403-42ae-e68e-896825318a9f"},"source":["restore_training = False\n","label_type = \"one_hot\"\n","n_classes = 5\n","seq_length = 50\n","augment = True\n","read_head_num = 4\n","batch_size = 16\n","num_epochs = 5000\n","learning_rate = 1e-3\n","rnn_size = 200\n","image_width = 20\n","image_height = 20\n","rnn_num_layers = 1\n","num_memory_slots = 128\n","memory_vector_dim = 40\n","shift_range = 1\n","write_head_num = 4\n","test_batch_num = 100\n","n_train_classes = 220\n","n_test_classes = 60\n","save_dir = './save/one_shot_learning'\n","tensorboard_dir = './summary/one_shot_learning'\n","model_path = 'LSTM'\n","train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-eb58e367ca65>:25: BasicLSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-3-eb58e367ca65>:26: MultiRNNCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","Model initialized\n","Entered Dataloader\n","10.0% data loaded.\n","20.0% data loaded.\n","30.0% data loaded.\n","40.0% data loaded.\n","50.0% data loaded.\n","60.0% data loaded.\n","70.0% data loaded.\n","80.0% data loaded.\n","90.0% data loaded.\n","100.0% data loaded.\n","Data loaded\n","1st\t2nd\t3rd\t4th\t5th\t6th\t7th\t8th\t9th\t10th\tepoch\tloss\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0\t81.6794\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t100\t80.7570\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t200\t80.6511\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t300\t80.5218\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t400\t80.6827\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t500\t80.4550\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t600\t80.5586\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t700\t80.4395\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t800\t80.5365\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t900\t80.5101\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1000\t80.5084\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1100\t80.5843\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1200\t80.6351\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1300\t80.4482\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1400\t80.5201\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1500\t80.4308\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1600\t80.5326\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1700\t80.5225\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1800\t80.4938\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t1900\t80.4940\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2000\t80.6110\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2100\t80.4978\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2200\t80.4605\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2300\t80.4128\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2400\t80.4614\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2500\t80.4635\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2600\t80.4563\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2700\t80.4469\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2800\t80.4535\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t2900\t80.4793\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3000\t80.4889\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3100\t80.4863\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3200\t80.4391\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3300\t80.4769\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3400\t80.4195\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3500\t80.4925\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3600\t80.4867\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3700\t80.4548\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3800\t80.4949\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t3900\t80.4769\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4000\t80.4290\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4100\t80.4683\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4200\t80.4456\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4300\t80.4306\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4400\t80.4506\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4500\t80.4885\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4600\t80.4585\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4700\t80.4448\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4800\t80.4618\n","0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t0.0000\t4900\t80.4477\n"],"name":"stdout"}]}]}