{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"one_shot_learning_Student_191111.ipynb","provenance":[{"file_id":"17BvkhrvTKBi0IMM1Fazl6UVLtm0fb8E7","timestamp":1573460220489},{"file_id":"1tRHaZoIYsfB16Gh-s_KAX6A41NdP3XKa","timestamp":1573332369516},{"file_id":"1EfocEfdlVns48iXbXJf7_Jy-v-WCN9NN","timestamp":1573327062889}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vjlCaEh7fGRF"},"source":["#Name: Justin Helfman"]},{"cell_type":"markdown","metadata":{"id":"Ep1Ky-nDBZIr"},"source":["You might need to modify the third line in the code cell below, to make sure you cd to the actual directory which your ipynb file is located in.\n","\n","**Caution**: due to the nature of this project's setup, everytime you want to rerun some code cell below, please click **Runtime -> Restart and run all**; this operation clears the computational graphs and the local variables but allow training and testing data that are already loaded from google drive to stay in the colab runtime space. Please do **not** do the following if you just wish to rerun code: click Runtime -> reset all runtimes, and then click Runtime -> Run all; it will remount your google drive, and remove the training and testing data already loaded in your colab runtime space. **Runtime -> Restart and run all** automatically avoids remounting the drive after the first time you run the notebook file; the loaded data can usually stay in your colab runtime space for many hours.\n","\n","Loading the training and testing data after remounting your google drive takes 30 - 40 minutes."]},{"cell_type":"code","metadata":{"id":"7sWni0FseVUz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607123126675,"user_tz":300,"elapsed":1409,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}},"outputId":"1b10f404-7056-460b-e991-b220f627e7ee"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\", force_remount=True)\n","%cd gdrive/MyDrive/ECE\\ 595/'Assignment 5'/Project_5_Student_code/NTM_Student\n","%cd Neural_Turing_Machine/NTM_small #These directories are not present (at least in the zip file I got)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n","/content/gdrive/MyDrive/ECE 595/Assignment 5/Project_5_Student_code/NTM_Student\n","[Errno 2] No such file or directory: 'Neural_Turing_Machine/NTM_small #These directories are not present (at least in the zip file I got)'\n","/content/gdrive/MyDrive/ECE 595/Assignment 5/Project_5_Student_code/NTM_Student\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R9gdekJg_-xa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607123128242,"user_tz":300,"elapsed":2963,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}},"outputId":"25ece247-a512-4873-e913-deb3a113b67b"},"source":["from utils import OmniglotDataLoader, one_hot_decode, five_hot_decode\n","import tensorflow as tf\n","import argparse\n","import numpy as np\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","%tensorflow_version 1.x\n","print(tf.__version__)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","TensorFlow is already loaded. Please restart the runtime to change versions.\n","2.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"efHLdzvkAKfK"},"source":["The following class `MANNCell` is the core of the memory-augmented neural network (MANN). You will implement the main parts of it in Tensorflow 2.0.\n","\n","Before any technical discussion of how the MANNCell should operate, let us look at what it should do on a general level. Suppose we have an input batch of 16 episodes of image samples, with each episode being of equal length of 50. Based on the design of the rest of the project (which we have already implemented for you), MANNCell should be called 50 times, each time having 16 input samples (along with the offseted labels), and outputting 16 output labels. More specifically, the MANNCell should produce classification labels $[\\hat{y}_0^t, ..., \\hat{y}_{15}^t]$ for all 16 iteration-$t$ image samples batch $[x^t_0+\\text{null}, x^t_1+y_0^t, ..., x^t_{15}+y_{14}^t]$ (\"+\" means concatenation) every time it is called; for your information, it is the class NTMOneShotLearningModel (already implemented below) that actually calls MANNCell 50 times. Your job is to make sure that at a single iteration $t$ (where $t=0,1,2,...,49$), MANNCell correctly parses the input arguments, produce the correct read and write weights $w^r_t, w^w_t$, correctly retrieve from and write to the memory to form $M_t$, and use the right material to get the logits for classification (they will be used for computing the labels and cross-entropy values in NTMOneShotLearningModel), and return the right states that will be used in the next iteration $t+1$. \n","\n","Let us look at the input arguments of the method `call(self, inputs, states)`  of this class first:\n","*   The `inputs` variable shall have the following shape: \n","    `(batch_size, image_size+num_classes)`. \n","  *   It corresponds to the $[x^t_0+\\text{null}, x^t_1+y^t_0, ..., x^t_{15}+y^t_{14}]$ above, for some iteration $t=0,1,...,49$.\n","  *   `inputs[p,:]` is the $p$-th image in the batch `inputs` (note that the images are flattened to 1D tensors, and the labels are one-hot encoded).\n","*   The `states` variable is a dictionary that has the following set of keys:`{'controller_state', 'read_vector_list', 'w_r_list', 'w_u', 'M'}`\n","  *   `controller_state` is the state of the controller in iteration $t-1$; if $t-1 < 0$, then it is just zero-filled. As it is an LSTM cell, `controller_state` is of the form `[(batch_size, rnn_size),(batch_size, rnn_size)]` (technically speaking its shape is `(2, batch_size, rnn_size)`). The two `(batch_size, rnn_size)`-shaped entries in it correspond to the cell state and the hidden state of the LSTM. We will mostly be treating the LSTM controller as a black-box in this project, so we do not need to pay much attention to the details of its states. If interested, you can read about the LSTM cell's technical details in [tf.keras.layers.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell).\n","  *   `read_vector_list` is the list of read vectors $r_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the read vector list is initialized to be an arbitrary one-hot vector. It is of the shape `(head_num, batch_size, memory_vector_dim)`. Basically, `read_vector_list[i,p,:]` is the $(t-1)$-th-iteration read vector of the $i$-th read head for the $p$-th input sample in the batch.\n","  *   `w_r_list` is the list of read weights $w^r_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the read weights list is initialized to be an arbitrary one-hot vector. It is of the shape `(head_num, batch_size, num_memory_slots)`. Basically, `w_r_list[i,p,:]` is the $(t-1)$-th-iteration read weight of the $i$-th read head for the $p$-th input sample in the batch.\n","  *   `w_u` is the list of memory usage weights $w^u_{t-1}$ which we obtained in the previous iteration $t-1$ in the episode; if $t-1 < 0$, then the usage weights list is is initialized to be an arbitrary one-hot vector. It is of the shape `(batch_size, num_memory_slots)`. Basically, `w_u[p,:]` is the $(t-1)$-th-iteration memory usage weight of the $p$-th input sample in the batch.\n","  *   `M` is the memory content from the previous iteration $t-1$; if $t-1 < 0$, then the memory is just zero-filled. It is of shape `(batch_size, num_memory_slots, memory_vector_dim)`. Basically, `M[p,j,:]` is the $j$-th memory vector in the memory block for the $p$-th sample in the batch from iteration $t-1$, and `M[p,:,:]` is the memory block for the $p$-th sample in the batch, where the memory block is a 2D structure that has `num_memory_slots` memory vectors, each vector of length `memory_vector_dim`.\n","\n","\n","\n","\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"ivOh5NlfRtC3"},"source":["\n","\n","---\n","\n","\n","\n","Now let us look at some of the technical details of the MANNCell. First, we discuss the main ingredients of the MANNCell, and initialization of the relevant units.\n","*   The input arguments of the class initialization method `__init__` have already been specified, they will be used to initialize relevant structures in the class.\n","*   `self.controller`: this is the controller of the MANN cell that is responsible for interfacing with the memory $M$. We recommend using `tf.keras.layers.LSTMCell` with `units=rnn_size` for initialization. For its technical details, see [tf.keras.layers.LSTMCell\n","](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell).\n","*   `self.controller_output_to_read_keys`, `self.controller_output_to_write_keys`, `self.controller_output_to_alphas`: the LSTM controller's output structure (we will discuss what its inputs should be later) is of the form [controller_output, controller_cell_and_hidden_states]. We need a mapping that maps the controller_output to the read keys, write keys and the interpolation coefficient $\\alpha_t$'s, which will then be used for interacting with the memory. Three `tf.keras.layers.Dense` layers (one for producing read keys, one for write keys, one for the $\\alpha_t$'s) are sufficient, though you are welcome to try out more complicated structures. \n","  *  **Remark 1**: each access to memory involves `head_num` number of heads, if you wish, you could just initialize `self.controller_output_to_read_keys` with `units=self.memory_vector_dim*self.head_num` and apply `tf.split` to the output of the dense layer along `axis=1` and `num_or_size_splits=head_num` in the `call` method (similar for the other two dense layers).\n","  *  ***From now on, we assume that you are following Remark 1 above in your implementation***.\n","*    `self.controller_output_to_logits`: it should be a dense layer that will be used to map the concatenated controller_output + read_vector_list to the logits that will be used for obtaining the classification labels of the inputs and computing the cross entropy values. Thus, initialize it with `units=self.num_classes`."]},{"cell_type":"markdown","metadata":{"id":"bMM2bJpSR3EP"},"source":["---\n","\n","Finally, we discuss how to implement the method `call`. The following discussion is only one way of implementing the method, please feel free to deviate from it. However, ***we do suggest you to at least read through the discussion once***, as we have already implemented parts of the method and the whole training loop for you, and incompatibility between the data structures could cause the code to not run or have buggy outputs.\n","*   **Caution**: even though most of the discussion below that involve tensors are treated either element-wise or vector-wise, in your implementation please utitlize tensorflow matrix operations as much as possible, as it can avoid strange bugs and increase the speed of your model.\n","*   As described before, the input arguments of the `call` method are `inputs` and `states`. \n","  *  Parse `state` to obtain `prev_controller_state`, `prev_read_vector_list`, `prev_w_r_list`, `prev_w_u`, `prev_M` that come from the previous iteration $t-1$. You may assume that they are zero-filled if $t=0$.\n","*  Constructing the controller's input had been implemented for you. \n","  *  The controller's output will be of the form `(controller_output, controller_states)`.\n","  *  Why do you think  we should involve `prev_read_vector_list` in the controller's input?\n","*  Now pass `controller_output` to the dense layers we discussed before, and obtain the read keys, write keys and the interpolation coefficients.\n","  * Following the suggestion in the Remark 1 above, after applying `tf.split` to the dense layers' outputs, the shapes of your `read_key_list` and `write_key_list` should both be `(head_num, batch_size, memory_vector_dim)`, and the shape of `alpha_list` should be `(head_num, batch_size, 1)`. As an example, `read_key_list[i,p,:]` should be the memory read key for the $i$-th read head for the $p$-th sample in the batch.\n","\n","*  Before computing the read and write weights and interact with memory, we need to compute `prev_w_lu`, the least used weights from the previous iteration $t-1$. \n","  *  You need to fill in the code for method `compute_w_lu`. To compute `prev_w_lu`, note that for the $p$-th sample in the batch in the previous iteration $t-1$, `prev_w_lu[p,:]` is a vector of binary values with length `num_memory_slots`: defining \n","     \\begin{equation}\n","      s(\\text{prev_w_u}[p,:], k)= \\text{the $k$-th smallest entry in prev_w_u}[p,:]\n","     \\end{equation}\n","     we have \n","     \\begin{equation}\n","      \\text{prev_w_lu}[p,i] = 0, \\;\\; \\text{if prev_w_u}[p,i] > s(\\text{prev_w_u}[p,:], \\text{head_num})\n","     \\end{equation}\n","     and \n","     \\begin{equation}\n","      \\text{prev_w_lu}[p,i]=1 \\;\\; \\text{otherwise}\n","     \\end{equation}\n","  *   Here is one way to implement `compute_w_lu`. Given input argument `prev_w_u` the usage weight from the previous iteration $t-1$ (it has shape `(batch_size, num_memory_slots)`), use `tf.math.top_k` to obtain the desired set of indices from `prev_w_u` (so you should have a `batch_size` number of index sets, each set is of size `head_num`; the overall structure should be of shape `(batch_size, head_num)`). Then use `tf.one_hot` and `tf.reduce_sum` to expand these indices into `prev_w_lu`, which should have shape `(batch_size, num_memory_slots)`. \n","    *  From the set of indices with size `(batch_size, head_num)` you used for computing `prev_w_lu`, remember to also construct and return the index corresponding to *the smallest* entry in `prev_w_u[p,:]` for every $p$ (this index also correspond to the memory slot that was least used for the $p$-th sample in the previous iteration); so your returned indices will have size `(batch_size, 1)`.\n","    *  You may find [tf.math.top_k\n","](https://www.tensorflow.org/api_docs/python/tf/math/top_k), [tf.one_hot\n","](https://www.tensorflow.org/api_docs/python/tf/one_hot) and [tf.reduce_sum\n","](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum)  useful.\n","    \n","\n","*  Now we proceed to compute the read and write weights $w^r_t$ and $w^w_t$.\n","  *  For the $p$-th sample in the batch, recall that the read key `read_key_list[m,p,:]` is for the $m$-th read head for that sample, and `prev_M[p,j,:]` is the $j$-th memory vector for the $p$-th sample from the previous interation $t-1$ . Then the memory **read** weight `w_r_list[m,p,:]` for the $m$-th read head for the $p$-th sample is a 1D tensor with length `num_memory_slots`, with entries\n","  \\begin{equation}\n","    \\text{w_r_list}[m,p,i] = \\frac{\\exp(K(\\text{prev_M}[p,i,:],\\text{read_key_list}[m,p,:]))}{\\sum_{j=0}^{\\text{num_memory_slots}-1}\\exp(K(\\text{prev_M}[p,j,:], \\text{read_key_list}[m,p,:]))}\n","  \\end{equation}\n","  where $i=0,1,...,\\text{num_memory_slots}-1\\$, and\n","  \\begin{equation}\n","    K(x, y) = \\frac{x\\cdot y}{\\Vert x \\Vert_2 \\Vert y \\Vert_2 + \\epsilon}\n","  \\end{equation}\n","    *  $\\epsilon$ is there to ensure numerical stability. $\\epsilon=10^{-8}$ seems to be a good choice.\n","    *  You might find some of the following tensorflow operations useful: [tf.matmul\n","](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul), [tf.norm\n","](https://www.tensorflow.org/api_docs/python/tf/norm), [tf.expand_dims\n","](https://www.tensorflow.org/api_docs/python/tf/expand_dims), [tf.squeeze\n","](https://www.tensorflow.org/api_docs/python/tf/squeeze), [tf.math.exp\n","](https://www.tensorflow.org/api_docs/python/tf/math/exp) \n","\n","    *  In the suggested setup, the method `compute_read_weights`'s return shape should be `(batch_size, num_memory_slots)`, and `w_r_list` should have shape `(head_num, batch_size, num_memory_slots)`.\n","\n","  *  Given the $p$-th sample in the batch, the memory **write** weight `w_w_list[m,p,:]` for the $m$-th write head for that sample is of the general form:\n","     \\begin{equation}\n","      \\text{w_w_list}[m,p,i] = \\text{Sigmoid}(\\text{alpha_list}[m,p,0])\\times\\text{prev_w_r_list}[m,p,i] + (1 - \\text{Sigmoid}(\\text{alpha_list}[m,p,0]))\\times\\text{prev_w_lu}[p,i]\n","     \\end{equation}\n","     where $i=0,...,\\text{num_memory_slots-1}$.\n","    *  In our suggested setup, method `compute_write_weights`'s return shape should be `(batch_size, num_memory_slots)`, so `w_w_list` should have shape `(head_num, batch_size, num_memory_slots)`.\n","\n","*  Let us read from memory `prev_M` now.\n","    *  As we have `w_r_list` with shape `(head_num, batch_size, num_memory_slots)`, to obtain the read vectors, simply carry out the following: for the $m$-th read head for the $p$-th sample, \n","      \\begin{equation}\n","        \\text{read_vector_list}[m,p,:] = \\sum_{j=0}^{\\text{num_memory_slots}-1}\\text{w_r_list}[m,p,j]\\times\\text{prev_M}[p,j,:]\n","      \\end{equation}\n","      where `read_vector_list` has shape `(head_num, batch_size, memory_vector_dim)`.\n","      *  Please remember that computing with matrices (in contrast to using some kind of for loop) can usually make you code run faster.\n","\n","* Having obtained the write weights `w_w_list`, we are closer to accessing the content of the memory now. But before that, rememeber that we got a set of indices of size `(batch_size, 1)` from the method `compute_w_lu` that indicated the least used memory slot in the previous iteration $t-1$? We are going to use them to zero out *the least used slot* in the memory first, before the writing operations.\n","  *  One way of implementation: apply `tf.one_hot` to the set of indices of size `(batch_size, 1)` to obtain a matrix `E` of size `(batch_size, num_memory_slots)` containing one-hot vectors, where `E[p,j]` is 1 if the $j$-th memory slot for the $p$-th sample in the previous iteration was least used. Then we just need to compute the new memory along the line of $M*(1-E)$. So we have obtained `M_erased`, with shape `(batch_size, num_memory_slots, memory_vector_dim)`.\n","\n","* Now we can write to memory:\n","  *  Recall that we have already computed `write_key_list` and `w_w_list` with shapes `(head_num, batch_size, memory_vector_dim)` and `(head_num, batch_size, num_memory_slots)` respectively. To write to `M_erased` with the $m$-th write head for the $p$-th sample, simply compute\n","     \\begin{equation}\n","      \\text{M_written}[p,i,:] = \\text{M_erased}[p,i,:] + \\text{w_w_list}[m,p,i]\\times\\text{write_key_list}[m,p,:]\n","     \\end{equation}\n","      *  You might find [tf.matmul\n","](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) and [tf.expand_dims\n","](https://www.tensorflow.org/api_docs/python/tf/expand_dims) useful here.\n","  \n","\n","*  Finally, update the usage weight $w^u_t$ following the formula: for the $p$-th sample in the batch,\n","   \\begin{equation}\n","     \\text{w_u}[p,:] = \\text{self.gamma}\\times\\text{prev_w_u}[p,:] + \\sum_{i=0}^{\\text{head_num}-1}\\text{w_r_list}[i,p,:] + \\sum_{i=0}^{\\text{head_num}-1}\\text{w_w_list}[i,p,:]\n","   \\end{equation}\n","   where `w_u` has shape `(batch_size, num_memory_slots)`, and `self.gamma` is a manually defined free parameter of the model, which we have already set for you.\n","*  Finally, we update the `state` dictionary , and feed [controller's output + the read vector list] to `self.controller_output_to_logits` which will be used for obtaining the labels for the input samples (already written for you) . Please ensure that all the relevant tensors have the correct shape and content.\n"]},{"cell_type":"markdown","metadata":{"id":"VORXI_ZUB5pT"},"source":["#Deliverables for Part 2:"]},{"cell_type":"markdown","metadata":{"id":"z-Dnff07B813"},"source":["[2]: NTM and MANN (Memory Augmented Neural Network):\n","\n","> [a]:Discuss differences between read, write and addressing implementations of NTM and MANN given in following two papers.\n","\n","> https://arxiv.org/pdf/1410.5401.pdf\n","\n","> http://proceedings.mlr.press/v48/santoro16.pdf\n","\n","> ANS: \n","With respect to the NTM and MANN papers provided, there were indeed a few differences made in adapting MANN from NTM.  Firstly, with respect to the operation of reading memory, the equations used for read vectors remained the same in adapting from NTM to MANN, however the read-write vector appeared to change (in MANN) by firstly using an exponential average according to softmax, and using an output layer according to softmax to be input to future states.  \n","\n","\n","With respect to the operation of writing, one of the main differences to come out of this is that the NTM paper tended to use an operaton with erase and add vectors, using each of these in their memory operations.  In MANN, however, an addition is made with respect to the addressing operation, which forces the write function to adapt accordingly.  Because of this, the MANN paper uses a different vector ki in its writing operations.\n","\n","\n","Finally, with respect to the addressing operation, the NTM paper uses two methods concurrently, one being addressing by content, and the other being by memory.  The main idea with these methods was that if the content addressing failed (i.e. similar content has already been stored) then the addressing by memory will serve as a backup.  MANN, however learns to solve this issue by implementing Least Recently Used Access (LRUA) which is content based memory addressing only.  With LRUA, addressing is interpolated between past read weights and weights scaled by usage weights.\n","\n","\n","> [b]:In general, is NTM one of the ways to implement MANN?\n","\n","> ANS:  In addressing the question of whether or not NTM can provide one of the ways to implement a MANN, I would say no.  With respect to the read and write operations, the NTM and MANN papers are fairly similar.  The real major difference between the two papers comes with the addressing operation which, though identical in the beginning, the papers diverge with memory operations that appear to operate fairly differently given a longer runtime (with a large amount of memory usage). "]},{"cell_type":"code","metadata":{"id":"Kwt3noNe_0PU","executionInfo":{"status":"ok","timestamp":1607123128483,"user_tz":300,"elapsed":3197,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}}},"source":["class MANNCell(tf.keras.layers.AbstractRNNCell):\n","  def __init__(self, rnn_size, num_memory_slots, memory_vector_dim, head_num, num_classes=5, gamma=0.95, **kwargs):\n","    super().__init__(**kwargs)\n","    ################ Setup ###############################################\n","    self.rnn_size = rnn_size\n","    self.num_memory_slots = num_memory_slots # number of memory slots\n","    self.memory_vector_dim = memory_vector_dim # size of each memory slot\n","    self.head_num = head_num\n","    self.write_head_num = head_num # memory access head number is the same for both read and write in our setup  \n","    self.gamma = gamma # decay parameter for computing the usage weights\n","\n","    self.num_classes = num_classes\n","    ########################################################################\n","\n","    # Controller RNN layer, we use an LSTM\n","    # Recommended: tf.keras.layers.LSTMCell\n","    self.controller = tf.keras.layers.LSTMCell(units=rnn_size) # Fill in\n","\n","    # controller_output \n","    #          -> read_key (batch_size, head_num*memory_vector_dim)\n","    #          -> write_key (batch_size, head_num*memory_vector_dim)\n","    #          -> alpha (batch_size, head_num), interpolation coefficient for writing to memory\n","    #\n","    # We suggest units=self.memory_vector_dim*self.head_num for initializing the dense layers\n","    # for read key and write keys, and units=self.head_num for the dense layer for alpha,\n","    # and apply tf.split along axis=1 in the call method\n","    self.controller_output_to_read_keys = tf.keras.layers.Dense(units=self.memory_vector_dim*self.head_num) # Fill in\n","    self.controller_output_to_write_keys = tf.keras.layers.Dense(units=self.memory_vector_dim*self.head_num) # Fill in\n","    self.controller_output_to_alpha = tf.keras.layers.Dense(units=self.head_num) # Fill in\n","\n","    # This is the dense layer for mapping the controller output + read vector list to \n","    # logits (which will then be used for computing the labels and cross-entropy values\n","    # in NTMOneShotLearningModel). So initialize it with units=self.num_classes.\n","    self.controller_output_to_logits = tf.keras.layers.Dense(units=self.num_classes)# Fill in\n","\n","  @property\n","  def state_size(self):\n","    return self.rnn_size\n","\n","  # This initializes the dictionary states in MANNCell, and returns the initial state.\n","  # Please do not change it.\n","  def zero_state(self, batch_size, rnn_size, dtype):\n","    one_hot_weight_vector = np.zeros([batch_size, self.num_memory_slots])\n","    one_hot_weight_vector[..., 0] = 1\n","    one_hot_weight_vector = tf.constant(one_hot_weight_vector, dtype=tf.float32)\n","    initial_state = {\n","            'controller_state': [tf.zeros((batch_size, rnn_size)), tf.zeros((batch_size, rnn_size))],\n","            'read_vector_list': [tf.zeros([batch_size, self.memory_vector_dim])\n","                                  for _ in range(self.head_num)],\n","            'w_r_list': [one_hot_weight_vector for _ in range(self.head_num)],\n","            'w_u': one_hot_weight_vector,\n","            'M': tf.constant(np.ones([batch_size, self.num_memory_slots, self.memory_vector_dim]) * 1e-6, dtype=tf.float32)\n","        }\n","    return initial_state\n","\n","  def call(self, inputs, states):\n","    # read vectors from the previous iteration, extract from states\n","    prev_read_vector_list =  states['read_vector_list'] # Fill in \n","    # state of controller from previous iteration t-1, extract from states\n","    prev_controller_state = states['controller_state'] # Fill in  \n","    # Obtain the list of w^r_{t-1}, M_{t-1}, and w^u_{t-1}, extract from states\n","    prev_w_r_list = states['w_r_list'] # Fill in\n","    prev_M = states['M'] # Fill in\n","    prev_w_u = states['w_u'] # Fill in\n","\n","    # Controller output form the parameters of the read and write vectors\n","    controller_input = tf.concat([inputs] + prev_read_vector_list, axis=1)\n","    controller_output, controller_state = self.controller(inputs=controller_input, states=prev_controller_state)\n","\n","    # Map the controller_output to the read_keys, write_keys, and alphas\n","    read_keys = self.controller_output_to_read_keys(inputs=controller_input) # Fill in\n","    write_keys = self.controller_output_to_write_keys(inputs=controller_input) # Fill in\n","    alphas = self.controller_output_to_alpha(inputs=controller_input) # Fill in\n","    \n","    # We have head_num heads per access to memory (same number of heads for read and write),\n","    # so split the parameters obtained above into head_num groups, \n","    # tf.split is useful here (try splitting along axis=1. Why?)\n","    \"\"\"\"\"\"\n","    read_key_list = tf.tanh(tf.split(read_keys, num_or_size_splits=self.head_num, axis=1))\n","    write_key_list = tf.tanh(tf.split(write_keys, num_or_size_splits=self.head_num, axis=1))\n","    sig_alpha = tf.sigmoid(tf.split(alphas, num_or_size_splits=self.head_num, axis=1))\n","\n","    # For every p-th sample in the batch (from iteration t-1), compute the index \n","    # corresponding to least used memory slot in prev_M[p,:,:], return as prev_indices.\n","    # Also compute w^lu_{t-1}, return as prev_w_lu.\n","\n","    \"\"\" Please fill in the method self.compute_w_lu.  \"\"\"\n","    prev_indices, prev_w_lu = self.compute_w_lu(prev_w_u)\n","    # Setup read and write weights\n","    w_r_list = []\n","    w_w_list = []\n","    # We obtain read and write weights for each head\n","    for i in range(self.head_num):\n","      # Obtain READ weights\n","      \"\"\"Please fill in the method self.compute_read_weights\"\"\"\n","      w_r = self.compute_read_weights(read_key_list[i], prev_M)\n","      # Obtain WRITE weights\n","      \"\"\"Please fill in the method self.compute_write_weights\"\"\"\n","      w_w = self.compute_write_weights(sig_alpha[i], prev_w_r_list[i], prev_w_lu)\n","      # Note: w_r_list is of shape (head_num, batch_size, num_memory_slots), and same for w_w_list\n","      w_r_list.append(w_r)\n","      w_w_list.append(w_w)\n","\n","\n","\n","    \"\"\"Read from memory M_{t-1}, using the w_r_list\"\"\"\n","    read_vector_list = []\n","    # Iterate over each head\n","    for i in (w_r_list):\n","        \"\"\"Fill in, compute read_vector read_vector_list should have shape (head_num, batch_size, memory_vector_dim)\"\"\"\n","        read_vector_list.append(i)\n","\n","    # Set least used memory slot in prev_M to ZERO, make use of prev_indices!\n","    M_erased = tf.one_hot(depth = 1, indices=[batch_size, num_memory_slots, memory_vector_dim]) #\"\"\"Fill in\"\"\"\n","\n","    # Write to memory, form M_t, using the w_w_list and write_keys\n","    # Iterate over each head\n","    for i in range(self.head_num):\n","      \"\"\"Fill in\"\"\"\n","      M_written = M_erased#[i,:] + tf.matmul(np.transpose(np.array(tf.split(write_key_list, num_or_size_splits=1, axis=1))), np.array(tf.split(write_key_list, num_or_size_splits=1, axis=1))) #\"\"\"Fill in\"\"\"\n","    \n","    # Compute usage weights w^u_t for the current iteration\n","    w_u = w_w_list #\"\"\"Fill in\"\"\"\n","\n","    # Concatenate controller's output and the read memory\n","    # content, they are then fed into a dense layer to obtain the logits,\n","    # which will be used for obtaininig labels and computing the  cross-entropy \n","    # values in NTMOneShotLearningModel below\n","    read_vector_list = [controller_output]\n","    \n","    mann_output = tf.concat([controller_output] + read_vector_list, axis=1)\n","    logits = self.controller_output_to_logits(mann_output) #\"\"\"Fill in\"\"\"\n","\n","    state = {\n","        'controller_state': controller_state,\n","        'read_vector_list': read_vector_list,\n","        'w_r_list': w_r_list,\n","        'w_w_list': w_w_list,\n","        'w_u': w_u,\n","        'M': M_written,\n","    }\n","    print(logits)\n","    return logits, state\n","\n","  def compute_read_weights(self, read_key, prev_M):\n","    \"\"\"Fill in\"\"\"\n","\n","    # Compute the inner products, norms\n","\n","    inner_prod = np.inner(np.array(tf.split(read_key, num_or_size_splits=1, axis=1)), np.array(tf.split(read_key, num_or_size_splits=1, axis=1))) #np.inner(np.array(tf.split(read_key, num_or_size_splits=1, axis=1)), np.array(tf.split(prev_M, num_or_size_splits=1, axis=1)))\n","    K = np.inner(np.array(tf.split(read_key, num_or_size_splits=1, axis=1)), np.array(tf.split(read_key, num_or_size_splits=1, axis=1))) / (tf.norm(read_key) * tf.norm(prev_M) + 10**-8) #np.inner(tf.split(read_key, num_or_size_splits=1, axis=1), tf.split(prev_M, num_or_size_splits=1, axis=1)) / (tf.norm(read_key) * tf.norm(prev_M) + 10**-8)\n","    K = np.array(tf.split(K, num_or_size_splits=1, axis=1))\n","    \n","    # Compute the exp(K(M,key))'s\n","\n","    power = [[]* len(K[0])] * len(K)\n","\n","    for j in range(len(K)):\n","      for i in range(len(K[j])):\n","        raised = tf.split(K[j], num_or_size_splits=1, axis=1)\n","        raised = np.array(raised)\n","        raised = np.exp(i)\n","        power[j].append(raised / j)\n","\n","    # Obtain read weights\n","    w_r = power\n","\n","    return w_r\n","\n","  def compute_write_weights(self, sig_alpha, prev_w_r, prev_w_lu):\n","    # Compute the write weights\n","    \"\"\"Fill in\"\"\"\n","    w_w = tf.sigmoid(sig_alpha) * prev_w_r + (1 - tf.sigmoid(sig_alpha)) * prev_w_lu\n","\n","    return w_w\n","\n","  def compute_w_lu(self, prev_w_u):\n","    \"\"\"Fill in\"\"\"\n","    indices = tf.math.top_k(prev_w_u)\n","    prev_w_lu = tf.one_hot(indices, depth=self.num_memory_slots) #HEREHEREHEREHEREHEREHERE\n","    indices = tf.reduce_sum(prev_w_lu)\n","    return indices, prev_w_lu\n","  "],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otNm4yidAQQB"},"source":["Already implemented, no need to change.\n","\n","This class is part of the training loop."]},{"cell_type":"code","metadata":{"id":"MZTXPodW_5_i","executionInfo":{"status":"ok","timestamp":1607123128483,"user_tz":300,"elapsed":3192,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}}},"source":["class NTMOneShotLearningModel():\n","  def __init__(self, model, n_classes, batch_size, seq_length, image_width, image_height,\n","                rnn_size, num_memory_slots, rnn_num_layers, read_head_num, write_head_num, memory_vector_dim, learning_rate):\n","    self.output_dim = n_classes\n","\n","    # Note: the images are flattened to 1D tensors\n","    # The input data structure is of the following form:\n","    # self.x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","    self.x_image = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, image_width * image_height])\n","    # Model's output label is one-hot encoded\n","    # The data structure is of the following form:\n","    # self.x_label[i,j,:] = one-hot label of the jth image in \n","    #             the ith sequence (or, episode)\n","    self.x_label = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","    # Target label is one-hot encoded\n","    self.y = tf.placeholder(dtype=tf.float32, shape=[batch_size, seq_length, self.output_dim])\n","\n","    if model == 'LSTM':\n","      # Using a LSTM layer to serve as the controller, no memory\n","      def rnn_cell(rnn_size):\n","        return tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n","      cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(rnn_size) for _ in range(rnn_num_layers)])\n","      state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n","    elif model == 'MANN':\n","      # Using a MANN network as the controller, with memory\n","      cell = MANNCell(rnn_size, num_memory_slots, memory_vector_dim,\n","                                head_num=read_head_num)\n","      state = cell.zero_state(batch_size=batch_size, rnn_size=rnn_size, dtype=tf.float32)\n","    \n","    \n","    self.state_list = [state]\n","    # Setup the NTM's output\n","    self.o = []\n","    \n","    # Now iterate over every sample in the sequence \n","    for t in range(seq_length):\n","      output, state = cell(tf.concat([self.x_image[:, t, :], self.x_label[:, t, :]], axis=1), state)\n","      output = tf.nn.softmax(output, axis=1)\n","      self.o.append(output)\n","      self.state_list.append(state)\n","    # post-process the output of the classifier\n","    self.o = tf.stack(self.o, axis=1)\n","    self.state_list.append(state)\n","\n","    eps = 1e-8\n","    # cross entropy, between model output labels and target labels\n","    self.learning_loss = -tf.reduce_mean(  \n","        tf.reduce_sum(self.y * tf.log(self.o + eps), axis=[1, 2])\n","    )\n","    \n","    self.o = tf.reshape(self.o, shape=[batch_size, seq_length, -1])\n","    self.learning_loss_summary = tf.summary.scalar('learning_loss', self.learning_loss)\n","\n","    with tf.variable_scope('optimizer'):\n","      self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","      self.train_op = self.optimizer.minimize(self.learning_loss)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_qMlbTWAvg0"},"source":["The training and testing functions"]},{"cell_type":"code","metadata":{"id":"Se1yEaxmey6Z","executionInfo":{"status":"ok","timestamp":1607123128484,"user_tz":300,"elapsed":3188,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}}},"source":["def train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir):\n","  \n","  # We always use one-hot encoding of the labels in this experiment\n","  label_type = \"one_hot\"\n","\n","  # Initialize the model\n","  model = NTMOneShotLearningModel(model=model_path, n_classes=n_classes,\\\n","                    batch_size=batch_size, seq_length=seq_length,\\\n","                    image_width=image_width, image_height=image_height, \\\n","                    rnn_size=rnn_size, num_memory_slots=num_memory_slots,\\\n","                    rnn_num_layers=rnn_num_layers, read_head_num=read_head_num,\\\n","                    write_head_num=write_head_num, memory_vector_dim=memory_vector_dim,\\\n","                    learning_rate=learning_rate)\n","  print(\"Model initialized\")\n","  data_loader = OmniglotDataLoader(\n","      image_size=(image_width, image_height),\n","      n_train_classses=n_train_classes,\n","      n_test_classes=n_test_classes\n","  )\n","  print(\"Data loaded\")\n","  # Note: our training loop is in the tensorflow 1.x style\n","  with tf.Session() as sess:\n","    if restore_training:\n","      saver = tf.train.Saver()\n","      ckpt = tf.train.get_checkpoint_state(save_dir + '/' + model_path)\n","      saver.restore(sess, ckpt.model_checkpoint_path)\n","    else:\n","      saver = tf.train.Saver(tf.global_variables())\n","      tf.global_variables_initializer().run()\n","    train_writer = tf.summary.FileWriter(tensorboard_dir + '/' + model_path, sess.graph)\n","    print(\"1st\\t2nd\\t3rd\\t4th\\t5th\\t6th\\t7th\\t8th\\t9th\\t10th\\tepoch\\tloss\")\n","    for b in range(num_epochs):\n","      # Test the model\n","      if b % 100 == 0:\n","        # Note: the images are flattened to 1D tensors\n","        # The input data structure is of the following form:\n","        # x_image[i,j,:] = jth image in the ith sequence (or, episode)\n","        # And the sequence of 50 images x_image[i,:,:] constitute\n","        # one episode, and each class (out of 5 classes) has around 10\n","        # appearances in this sequence, as seq_length = 50 and \n","        # n_classes = 5, as specified in the code block below\n","        # See the details in utils.py, OmniglotDataLoader class\n","        x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length,\n","                                  type='test',\n","                                  augment=augment,\n","                                  label_type=label_type)\n","        feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","        output, learning_loss = sess.run([model.o, model.learning_loss], feed_dict=feed_dict)\n","        merged_summary = sess.run(model.learning_loss_summary, feed_dict=feed_dict)\n","        train_writer.add_summary(merged_summary, b)\n","        accuracy = test(seq_length, y, output)\n","        for accu in accuracy:\n","          print('%.4f' % accu, end='\\t')\n","        print('%d\\t%.4f' % (b, learning_loss))\n","\n","      # Save model per 2000 epochs\n","      if b%2000==0 and b>0:\n","        saver.save(sess, save_dir + '/' + model_path + '/model.tfmodel', global_step=b)\n","\n","      # Train the model\n","      x_image, x_label, y = data_loader.fetch_batch(n_classes, batch_size, seq_length, \\\n","                                type='train',\n","                                augment=augment,\n","                                label_type=label_type)\n","      feed_dict = {model.x_image: x_image, model.x_label: x_label, model.y: y}\n","      sess.run(model.train_op, feed_dict=feed_dict)\n","\n","# Fill in this function. You might not need seq_length (the length of an episode)\n","# as an input, depending on your setup \n","# Note: y is the true labels, and of shape (batch_size, seq_length, 5)\n","# output is the network's classification labels\n","def test(seq_length, y, output):\n","  # Fill in\n","  c = [[] * seq_length] * 10\n","\n","  i = 0\n","\n","  for i in range(10):\n","    for j in range(seq_length):\n","      mxdex = list(y[i][j]).index(min(y[i][j]))\n","      if output[i][j][mxdex] == 1:\n","        c[i].append(1)\n","      else:\n","        c[i].append(0)\n","\n","  acc = [sum(c[j])/sum(sum(y[j])) for j in range(10)]\n","\n","  return acc\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"VruOInLHkZUK","colab":{"base_uri":"https://localhost:8080/","height":891},"executionInfo":{"status":"error","timestamp":1607123129744,"user_tz":300,"elapsed":4442,"user":{"displayName":"Justin Helfman","photoUrl":"","userId":"11136277895536242776"}},"outputId":"12ffe68d-aeee-4db1-f34c-e05ecd9a9574"},"source":["restore_training = False\n","label_type = \"one_hot\"\n","n_classes = 5\n","seq_length = 50\n","augment = True\n","read_head_num = 4\n","batch_size = 16\n","num_epochs = 100000\n","learning_rate = 1e-3\n","rnn_size = 200\n","image_width = 20\n","image_height = 20\n","rnn_num_layers = 1\n","num_memory_slots = 128\n","memory_vector_dim = 40\n","shift_range = 1\n","write_head_num = 4\n","test_batch_num = 100\n","n_train_classes = 220\n","n_test_classes = 60\n","save_dir = './save/one_shot_learning'\n","tensorboard_dir = './summary/one_shot_learning'\n","model_path = 'MANN'\n","train(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, \\\n","         num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Tensor(\"mann_cell/concat:0\", shape=(16, 565), dtype=float32)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"mann_cell/dense_3/BiasAdd:0\", shape=(16, 5), dtype=float32)\n","Tensor(\"mann_cell_1/concat:0\", shape=(16, 605), dtype=float32)\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-371c5f712d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtensorboard_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./summary/one_shot_learning'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MANN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_training\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_memory_slots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-b425a30b11de>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(learning_rate, image_width, image_height, n_train_classes, n_test_classes, restore_training, num_epochs, n_classes, batch_size, seq_length, num_memory_slots, augment, save_dir, model_path, tensorboard_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNTMOneShotLearningModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mimage_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mrnn_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_memory_slots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_memory_slots\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mrnn_num_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_num_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_head_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_head_num\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mwrite_head_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_head_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_vector_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_vector_dim\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model initialized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   data_loader = OmniglotDataLoader(\n","\u001b[0;32m<ipython-input-4-841069d18f7b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, n_classes, batch_size, seq_length, image_width, image_height, rnn_size, num_memory_slots, rnn_num_layers, read_head_num, write_head_num, memory_vector_dim, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Now iterate over every sample in the sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m               \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-3-356200202858>:71 call  *\n        controller_output, controller_state = self.controller(inputs=controller_input, states=prev_controller_state)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:776 __call__  **\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py:2442 call\n        x_i = K.dot(inputs_i, k_i)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:1831 dot\n        out = math_ops.matmul(x, y)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3255 matmul\n        a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:5642 mat_mul\n        name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 605 and 565 for '{{node mann_cell_1/lstm_cell/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false](mann_cell_1/concat, mann_cell_1/lstm_cell/split)' with input shapes: [16,605], [565,200].\n"]}]},{"cell_type":"markdown","metadata":{"id":"1XBqApyFQSiL"},"source":["######4. Report the best ACC(1st instance), ..., ACC(10th instance) that your NTM model can reach.You should be able to outperform the simple LSTM network from part 1 by a large marginin around 10,000 epochs of training, but feel free to train it for less epochs.Remark: In the first few thousand of epochs, the model’s loss could stay very high with littlechange.  Please have patience, if your implementation is correct, it should most likely startdecreasing before you reach 10,000 epochs of training.   Please note that it could happenthat your implementation is correct but the loss just does not change (training this networkrequires a bit of luck!);  we suggest restarting the training (so with a different random ini-tialization of the weights) if you do not see the loss changing for 5,000 to 7,000 epochs; youmight need to check your code though if a number of restarts still do not help.\n","\n","[ANS]: Of course with respect to this project, I was not able to complete the exercise and faced major issues in trying to get the code to run.  After many attempts, I tried to fudge things a little (as can be seen in the code) in an attempt to get the code to execute, but this was to no avail.  For the below 3 questions, I will be attempting to make predictions, had my code run, however for this part, I cannot report any ACC values, because my code has (of course) not been able to run.  Given my training performance on the first project though, I would gess the values would be all 0.\n","\n","######5.  Having gone through the exercise, in your opinion, does every step in our implementationmake sense?  For instance, do you think if we should have used more complicated struc-tures  for  computing  the  read  keys,  write  keys  and  the  interpolation  coefficients?   Maybewe should have written to the memory before reading from it?  Maybe a better measure ofsimilarity than the cosine similarity in computing the reading weights?  Does zeroing outthe least used memory slot from before really make sense?  A better mechanism of writingto memory than the “least recently used access”?  Is there any hope of utilizing the experi-ence and intuition we gained about NTMs and the one-shot learning task here on normalclassification problems? Please elaborate.\n","\n","[ANS]:  As you may probably guess, not every step really made sense.  I wish I was able to test things bit-by bit (which I believe to be a major issue), as I couldn't run the code until everything was filled in, and this made it harder to debug as there were far more places where things could go wrong.  In terms of the read write keys and interpolation coefficients, I believe:\n","\n","- The suggestion of writing to memory before reading would be very memory intensive, and may force an unnecessary dependency on prior episodes (especially those so far in the past that they may be irrelevant).\n","- The suggestion of a better measure of similarity (than cosine) for reading weights may not be necessary.  Given this problem, the cosine similarity metric does seem to be a simple metric between points, and doesn't seem to be a poor similarity metric.\n","- The suggestion of zeroing out the least used memory slot could make sense, generally speaking.  In scenarios where the least used memory slot is far less than the average memory slot usage, this could improve generalization performance, however in cases where the least used memory slot is somewhat close to the average memory slot usage, this could lead to important information being lost (almost like a catastrophic forgetting scenario).\n","- The suggestion of a better mechanism of writing to memory than LRUA could theoretically exist, but given the improvement from the original combination of content and memory addressing, this may not be a necessary step in terms of improving performance.\n","- The suggestion of utulizing NTMs and one-shot learning on normal classification problems is a potentially good one.  Though (of course) the implementation of these are a bit complex, the usage of NTMs would be fairly successful in larger-scaled classification problems.  In much smaller scenarios, however, it may be somewhat unnecessary especially when considering memory usage.\n","\n","######6.  If you are curious, try to sample a few episodes from the dataset, each episode of length 50,and test yourself on them. Can you outperform the NTM?\n","\n","[ANS]: Though I cannot perform this given the state of my code, I would assume that the NTM would not be outperformed, being consistent with my prediction in the previous question.\n"]}]}